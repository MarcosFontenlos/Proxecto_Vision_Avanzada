# ðŸ¤– Proxecto VisiÃ³n Artificial Avanzada

Este proyecto desarrolla un sistema de percepciÃ³n visual modular sobre **ROS 2**, capaz de detectar personas y gestos mediante visiÃ³n artificial, y controlar el movimiento de un robot mÃ³vil (Kompai) en funciÃ³n de la informaciÃ³n captada.

## ðŸ§  Arquitectura del Sistema

El sistema estÃ¡ compuesto por tres nodos principales:

* ðŸŸ¡ **`gesture_movimiento.py`**
  VersiÃ³n inicial con detecciÃ³n de gestos de mano mediante MediaPipe y seguimiento de personas usando YOLOv8, ademÃ¡s tambiÃ©n se integrarÃ¡ la deteccion de botellas. Publica comandos en `/robulab10/cmd_vel`.

* ðŸŸ¡ **`gesture_segmentacion.py`**
  Combina detecciÃ³n de gestos con MediaPipe y seguimiento visual avanzado de personas y botellas con YOLOv8 + segmentaciÃ³n. Usa tambiÃ©n un sistema de cambio de estado basado en gestos y lÃ³gica de transiciÃ³n inteligente.

## ðŸ“ Estructura del Workspace

```
Proxecto_Vision_Avanzada/
â”œâ”€â”€ gesture_movimiento.py               # VersiÃ³n inicial (YOLO + MediaPipe)
â”œâ”€â”€ gesture_segmentacion.py            # VersiÃ³n secundaria (YOLO + segmentaciÃ³n + gestos)
â”œâ”€â”€ gesture_segmentacion_CamaraLocal.py
â”œâ”€â”€ camara_ip.py                       # Captura por IP
â”œâ”€â”€ yolov8n.pt                         # Modelo YOLOv8 Nano (personas)
â”œâ”€â”€ yolov8n-seg.pt                     # Modelo YOLOv8 Nano SegmentaciÃ³n (botellas)
â”œâ”€â”€ yolov8m.pt                         # Modelo YOLOv8 Medium (alternativo)
â”œâ”€â”€ metricas.csv                       # MÃ©tricas automÃ¡ticas
â”œâ”€â”€ metricas_rendimiento.csv           # MÃ©tricas personalizadas
â”œâ”€â”€ Dockerfile                         # Imagen base con ROS + YOLO + OpenCV
â”œâ”€â”€ README.md                          # Este documento
â””â”€â”€ Proposta_Traballo/
    â”œâ”€â”€ PropostaTaballoFM_plantilla.tex # Memoria en LaTeX
    â””â”€â”€ PropostaTaballoFM_plantilla.pdf # PDF generado
```

## âš™ï¸ InstalaciÃ³n

1. Clonar el repositorio:

```bash
git clone https://github.com/MarcosFontenlos/Proxecto_Vision_Avanzada.git
cd Proxecto_Vision_Avanzada
```

2. Instalar dependencias (si **no** usas Docker):

```bash
pip3 install ultralytics opencv-python mediapipe
```

3. O ejecutar dentro del Docker preparado (recomendado), debes modificar ROS_IP y device /dev/video... por el numero asociado a la cÃ¡mara:

```bash
xhost +SI:localuser:root && echo $DISPLAY && docker run -it --rm \
  --network=host \
  --gpus all \
  -e DISPLAY=$DISPLAY \
  -e QT_X11_NO_MITSHM=1 \
  -e ROS_MASTER_URI=http://192.168.1.30:11311 \
  -e ROS_IP=192.168.1.188 \
  -v /tmp/.X11-unix:/tmp/.X11-unix:rw \
  -v /home/alex/Desktop/Vision_Artificial_Avanzada/Proyecto/Proxecto_Vision_Avanzada:/catkin_ws/src \
  --device /dev/dri:/dev/dri \
  --device /dev/video2:/dev/video2 \
  marcosfontenlos/ros_vision_base:ws3
```

## â–¶ï¸ EjecuciÃ³n

### âœ… Opcion 1: 

```bash
python3 gesture_movimiento.py --cam-index 2
```

### âœ… Opcion 2

```bash
python3 gesture_segmentacion.py --cam-index 2
```

> Puedes usar `--ip` para conectar directamente a la Axis Camera:

```bash
python3 gesture_segmentacion.py --ip
```

---

## ðŸ•¹ ConexiÃ³n y Movimiento

### ðŸ”Œ ConexiÃ³n:

* Pulsar el botÃ³n en la parte trasera del mando.
* Pulsar **Start**.
* Esperar a que se encienda la luz.

### ðŸ—º Movimiento:

* Pulsar **gatillo izquierdo (L2)**.
* Mantener presionado **botÃ³n A**.
* Usar **joystick izquierdo** para desplazarse.

## ðŸ“¡ Topics utilizados

* `/robulab10/cmd_vel`  â†’ Movimiento del robot (Twist).
* `/detecciones_yolo`   â†’ Detecta personas/objetos (futuro).
* `/gestos_mano`        â†’ Gestos detectados (futuro).

## ðŸ›  TecnologÃ­as

* ROS Noetic
* Python 3.10
* YOLOv8 (Ultralytics)
* OpenCV
* MediaPipe 
* ComunicaciÃ³n HTTP/TCP con robot Kompai

## ðŸ‘¥ Autores

* **Marcos Fontenlos**
  [@MarcosFontenlos](https://github.com/MarcosFontenlos)

* **Alejandro Solar**
  [@AlejandroSIRob](https://github.com/AlejandroSIRob)

